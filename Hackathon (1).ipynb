{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab5e6f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices of anomalous patterns: [  2   6  10  41  48  49  58  62  66  74  81  89  91  94  95  97 103 106\n",
      " 135 143 162 168 179 189 202 203 204 206 209 210 214 220 239 255 283 285\n",
      " 302 303 333 339 343 345 359 365 373 377 380 390 402 404 442 459 465 471\n",
      " 483 485 486 505 540 543 547 557 567 582 595 601 611 613 615 619 680 681\n",
      " 700 702 705 737 739 742 748 782 790 791 807 811 813 817 819 840 865 877\n",
      " 910 935 945 948 949 951 960 968 979 995]\n"
     ]
    }
   ],
   "source": [
    "#Consider using machine learning algorithms to learn from past user behavior and identify anomalous patterns.\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Generate some sample data (replace this with your actual dataset)\n",
    "# Let's assume each row represents a user's behavior features\n",
    "# Features could include things like time spent on site, number of clicks, etc.\n",
    "# Here, we generate random data for demonstration purposes\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "n_features = 5\n",
    "X = np.random.rand(n_samples, n_features)\n",
    "\n",
    "# Create and fit the isolation forest model\n",
    "clf = IsolationForest(contamination=0.1)  # Adjust contamination based on expected percentage of anomalies\n",
    "clf.fit(X)\n",
    "\n",
    "# Predict anomalies (outliers)\n",
    "y_pred = clf.predict(X)\n",
    "\n",
    "# Identify anomalies\n",
    "anomalies_indices = np.where(y_pred == -1)[0]\n",
    "\n",
    "# Print indices of anomalous patterns\n",
    "print(\"Indices of anomalous patterns:\", anomalies_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6799f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#Rule Generation Develop pre-defined rules based on data access restrictions, keyword usage patterns, \n",
    "#and suspicious API activity.\n",
    "# Example data: data access logs, keyword usage patterns, and API activity\n",
    "data_access_logs = {\n",
    "    'user_id': [1, 2, 3, 4, 5],\n",
    "    'file_accessed': ['file1.txt', 'file2.txt', 'file3.txt', 'file1.txt', 'file4.txt'],\n",
    "    'access_timestamp': ['2024-04-01 08:30:00', '2024-04-01 10:45:00', '2024-04-01 12:15:00', '2024-04-02 09:30:00', '2024-04-02 13:45:00']\n",
    "}\n",
    "\n",
    "keyword_usage_patterns = {\n",
    "    'user_id': [1, 2, 3, 4, 5],\n",
    "    'search_query': ['sensitive data', 'financial information', 'confidential document', 'password reset', 'suspicious activity']\n",
    "}\n",
    "\n",
    "api_activity = {\n",
    "    'user_id': [1, 2, 3, 4, 5],\n",
    "    'api_calls': ['get_data', 'get_data', 'update_data', 'get_data', 'update_data']\n",
    "}\n",
    "\n",
    "# Convert data to pandas dataframes for easier manipulation\n",
    "df_logs = pd.DataFrame(data_access_logs)\n",
    "df_keywords = pd.DataFrame(keyword_usage_patterns)\n",
    "df_api = pd.DataFrame(api_activity)\n",
    "\n",
    "# Rule generation based on data access restrictions\n",
    "restricted_files = df_logs.groupby('file_accessed').filter(lambda x: len(x) > 2)['file_accessed'].unique()\n",
    "data_access_rules = [f\"Allow access to {file}\" for file in restricted_files]\n",
    "\n",
    "# Rule generation based on keyword usage patterns\n",
    "suspicious_queries = df_keywords[df_keywords['search_query'].str.contains('sensitive|confidential|password|suspicious')]\n",
    "keyword_rules = [f\"Alert on user {row['user_id']}: Suspicious keyword usage - '{row['search_query']}'\" for _, row in suspicious_queries.iterrows()]\n",
    "\n",
    "# Rule generation based on suspicious API activity\n",
    "suspicious_api_calls = df_api.groupby('api_calls').filter(lambda x: len(x) > 2)['api_calls'].unique()\n",
    "api_rules = [f\"Alert on user: Suspicious API activity - {api}\" for api in suspicious_api_calls]\n",
    "\n",
    "# Print generated rules\n",
    "print(\"Data Access Rules:\")\n",
    "for rule in data_access_rules:\n",
    "    print(rule)\n",
    "\n",
    "print(\"\\nKeyword Usage Rules:\")\n",
    "for rule in keyword_rules:\n",
    "    print(rule)\n",
    "\n",
    "print(\"\\nAPI Activity Rules:\")\n",
    "for rule in api_rules:\n",
    "    print(rule)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0490dcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "#:Cloud-based serverless functions for real-time data processing and rule triggering.\n",
    "#import these while running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3452ea60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5fd0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize AWS services\n",
    "sns_client = boto3.client('sns')\n",
    "\n",
    "# Define your rule-based processing logic\n",
    "def process_data(event, context):\n",
    "    # Parse incoming data from the event\n",
    "    data = json.loads(event['body'])\n",
    "    \n",
    "    # Example rule: If data value exceeds a threshold, trigger an alert\n",
    "    if data['value'] > 100:\n",
    "        # Send alert via SNS\n",
    "        sns_client.publish(\n",
    "            TopicArn='YOUR_SNS_TOPIC_ARN',\n",
    "            Subject='Alert: Threshold Exceeded',\n",
    "            Message=f\"Value {data['value']} exceeds threshold!\"\n",
    "        )\n",
    "        \n",
    "    # Example rule: If specific keyword is found in data, trigger an alert\n",
    "    if 'suspicious' in data['message']:\n",
    "        # Send alert via SNS\n",
    "        sns_client.publish(\n",
    "            TopicArn='YOUR_SNS_TOPIC_ARN',\n",
    "            Subject='Alert: Suspicious Keyword Detected',\n",
    "            Message=f\"Suspicious keyword found: {data['message']}\"\n",
    "        )\n",
    "        \n",
    "    # You can add more rules here\n",
    "    \n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': json.dumps('Data processed successfully')\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ff676e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f94e6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Automated Rule Engine, Development Tools Programming languages like Python with libraries such as Pandas\n",
    "#for data analysis and Scikit-learn for machine learning-based anomaly detection.\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Load data (replace this with your actual dataset)\n",
    "data = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Define functions for rule generation\n",
    "def generate_data_access_rules(data):\n",
    "    restricted_files = data['file_accessed'].value_counts()[data['file_accessed'].value_counts() > 2].index.tolist()\n",
    "    return [f\"Allow access to {file}\" for file in restricted_files]\n",
    "\n",
    "def generate_keyword_rules(data):\n",
    "    suspicious_keywords = ['sensitive', 'confidential', 'password', 'suspicious']\n",
    "    keyword_rules = []\n",
    "    for keyword in suspicious_keywords:\n",
    "        rows = data[data['search_query'].str.contains(keyword, case=False)]\n",
    "        if not rows.empty:\n",
    "            for _, row in rows.iterrows():\n",
    "                keyword_rules.append(f\"Alert on user {row['user_id']}: Suspicious keyword usage - '{row['search_query']}'\")\n",
    "    return keyword_rules\n",
    "\n",
    "def generate_api_rules(data):\n",
    "    api_calls = data['api_calls'].value_counts()[data['api_calls'].value_counts() > 2].index.tolist()\n",
    "    return [f\"Alert on user: Suspicious API activity - {api}\" for api in api_calls]\n",
    "\n",
    "# Generate rules\n",
    "data_access_rules = generate_data_access_rules(data)\n",
    "keyword_rules = generate_keyword_rules(data)\n",
    "api_rules = generate_api_rules(data)\n",
    "\n",
    "# Print generated rules\n",
    "print(\"Data Access Rules:\")\n",
    "for rule in data_access_rules:\n",
    "    print(rule)\n",
    "\n",
    "print(\"\\nKeyword Usage Rules:\")\n",
    "for rule in keyword_rules:\n",
    "    print(rule)\n",
    "\n",
    "print(\"\\nAPI Activity Rules:\")\n",
    "for rule in api_rules:\n",
    "    print(rule)\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Load data (replace this with your actual dataset)\n",
    "data = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Define functions for rule generation\n",
    "def generate_data_access_rules(data):\n",
    "    restricted_files = data['file_accessed'].value_counts()[data['file_accessed'].value_counts() > 2].index.tolist()\n",
    "    return [f\"Allow access to {file}\" for file in restricted_files]\n",
    "\n",
    "def generate_keyword_rules(data):\n",
    "    suspicious_keywords = ['sensitive', 'confidential', 'password', 'suspicious']\n",
    "    keyword_rules = []\n",
    "    for keyword in suspicious_keywords:\n",
    "        rows = data[data['search_query'].str.contains(keyword, case=False)]\n",
    "        if not rows.empty:\n",
    "            for _, row in rows.iterrows():\n",
    "                keyword_rules.append(f\"Alert on user {row['user_id']}: Suspicious keyword usage - '{row['search_query']}'\")\n",
    "    return keyword_rules\n",
    "\n",
    "def generate_api_rules(data):\n",
    "    api_calls = data['api_calls'].value_counts()[data['api_calls'].value_counts() > 2].index.tolist()\n",
    "    return [f\"Alert on user: Suspicious API activity - {api}\" for api in api_calls]\n",
    "\n",
    "# Generate rules\n",
    "data_access_rules = generate_data_access_rules(data)\n",
    "keyword_rules = generate_keyword_rules(data)\n",
    "api_rules = generate_api_rules(data)\n",
    "\n",
    "# Print generated rules\n",
    "print(\"Data Access Rules:\")\n",
    "for rule in data_access_rules:\n",
    "    print(rule)\n",
    "\n",
    "print(\"\\nKeyword Usage Rules:\")\n",
    "for rule in keyword_rules:\n",
    "    print(rule)\n",
    "\n",
    "print(\"\\nAPI Activity Rules:\")\n",
    "for rule in api_rules:\n",
    "    print(rule)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d555114f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
